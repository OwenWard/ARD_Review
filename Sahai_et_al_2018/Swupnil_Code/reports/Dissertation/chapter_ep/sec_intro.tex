\section{Introduction}
\label{sec:ep_introduction}

Distributed algorithms are a natural approach for efficiently analyzing big data sets in a Bayesian context. In particular, expectation propagation (EP), first introduced by \citet{Minka:2001a}, has shown how information from data pieces, or sites, can be iteratively processed to arrive at the posterior distribution while decreasing the overall computational effort. The classical idea of processing one data point per site, however, is just one extreme of the distributed algorithm spectrum. While this approach provides massive computational gains over processing the entire data set at once, it also suffers from a lack of information from the other data pieces as a way to regularize the results from each site's inference. As such, grouping multiple data points into each site is a half-way point that allows ample information sharing while still providing computational advantages over the full data approach.

This framework has many real world applications, particularly when the data have a naturally partitioned structure. For example, in astronomical models of the Big Bang, radiation emitted from various regions of the universe can be analyzed to determine the age of the objects emitting such radiation. With deep spatial correlation of the radiation, it is valuable to pool the parameters of a generative radiation model together. With a fine grained map of the visible universe, however, the number of groups, and thus the number of local parameters, can be significantly large. When Bayesian methods such as MCMC are used to conduct inference on such models, the computation can become especially intractable given the large number of parameters. In such situations, splitting the groups of data and analyzing them in parallel can provide meaningful computational gains. At the same time, care must be taken to ensure that information is shared between groups to properly regularize the inference.

In this paper, we present an efficient distributed approach for hierarchical models, which by construction partition the data into conditionally separate pieces. In particular, we use the idea of EP's \emph{cavity distribution}, which approximates the effect of inferences from all other $K-1$ data partitions, as a prior in the inference step for individual partitions. With the framework outlined, we implement an example algorithm using the Stan probabilistic programming language \citep{Stan:2016} for the approximation steps, and we leverage its sample-based inferences for the individual partitions.

We then demonstrate the example algorithm's effectiveness on four data sets. The first three are synthetic data sets simulated from three increasingly complex hierarchical models. The fourth data set is an actual data set from astronomy, whose modeling requires fitting a complex hierarchical mlixutre model with $9$ local parameters for each of $360$ groups. We show how the EP approach provides massive computational gains over the full MCMC implementation for each of these models, even when EP runs in serial. We also demonstrate that for sufficiently complex models, serial EP is outperformed by full MCMC; however, in this case distributed EP still provides massive computational gains over full MCMC. We also discover that increase in number of sites eventually leads to a drop in posterior approximation accuracy.

The remainder of the paper proceeds as follows. We first review the hierarchical EP algorithm, demonstrate its applicability to partitioned data and hierarchical models, and discuss algorithmic considerations in Section \ref{sec:ep_ep}. Section \ref{sec:ep_application} then outlines a series of increasingly complex models fit by distributed EP to three synthetic data sets and one actual data set from astronomy, while Section \ref{sec:ep_results} presents the computational results as well as local parameter fits to these four data sets. Lastly, Section \ref{sec:ep_discussion} concludes the paper with a discussion.