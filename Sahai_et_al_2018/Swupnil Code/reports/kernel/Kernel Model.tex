\documentclass[12pt]{article}
\usepackage{amssymb,amsmath,graphicx,mathtools}
\usepackage{listings}
\usepackage[margin=0.75in]{geometry}
\parindent 16 pt
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[R]{Swupnil Sahai}
\fancyhead[C]{11/30/16}
\fancyhead[L]{Kernel Model}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\lstset{
    language=R,
    basicstyle=\scriptsize\ttfamily,
    stepnumber=1,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    frame=single,
    tabsize=2,
    captionpos=b,
    breaklines=true,
    breakatwhitespace=false,
    escapeinside={},
    keywordstyle={},
    morekeywords={}
    }

\begin{document}

% CUSTOM SHORTCUTS

\def\ci{\perp\!\!\!\perp}
\def\ex{\mathbb{E}}
\def\prob{\mathbb{P}}
\def\ind{\mathbb{I}}
\def\grad{\triangledown}
\def\bigo{\mathcal{O}}
\def\normal{\mathcal{N}}
\def\lognormal{\log\normal}

% INTRO %
\section{Introduction}
The class of models for aggregate relational data that we consider all involve modeling responses from a negative binomial distribution, with the mean $\mu_{ik}$ equal to the number of expected response for individual $i$ about knowing a number of people in a group of interest $\mathcal{G}_k$.
\begin{align}
y_{ik} &\sim \text{NegBin}(\omega_k \mu_{ik}, \omega_k) && \\\nonumber
E(y_{ik}) &= \mu_{ik} \\\nonumber
Var(y_{ik}) &= \mu_{ik} + \frac{\mu_{ik}}{\omega_k}
\end{align}

% Random Mixing %
\subsection{Random Mixing}
\noindent The most basic model treats this expectation as simply $d_i$, the degree of individual $i$, multiplied by the proportion of the population that is in $\mathcal{G}_k$. Letting $\delta_{jk} = \ind\{ j \in \mathcal{G}_k\}$, we can derive this expression as follows:
\begin{align}
\mu_{ik} 
&= \sum_{j=1}^{d_i} \ex[ \delta_{jk} | i \to j ] && \\\nonumber
&= \sum_{j=1}^{d_i} \prob( j \in \mathcal{G}_k | i \to j ) && \\\nonumber
&= d_i \prob( j \in \mathcal{G}_k | i \to j ) && \\\nonumber
&= d_i \prob( j \in \mathcal{G}_k) && \\\nonumber
&= d_i \biggl( \frac{N_k}{N} \biggr)
\end{align}

% NON-RANDOM AGE GENDER MIXING %
\pagebreak
\subsection{Non-Random Age and Gender Mixing}
If we believe that egos of certain ages and genders mix differently with alters of other ages and genders, then, we can model non-random age and gender mixing. Suppose the each individual $i$ belongs to some age category $a_i$ (e.g. 0-17, 18-24, etc.).
\begin{align}
\mu_{ik} 
&= \sum_{j=1}^{d_i} \prob( j \in \mathcal{G}_k | a_i, g_i, i \to j ) && \\\nonumber
&= d_i \prob( j \in \mathcal{G}_k | a_i, g_i, i \to j) && \\\nonumber
&= d_i \sum_{a_j,g_j} \prob( j \in \mathcal{G}_k, a_j, g_j | a_i, g_i, i \to j) && \\\nonumber
&= d_i \sum_{a_j,g_j} \prob( j \in \mathcal{G}_k | a_j, g_j, a_i, g_i, i \to j) p(a_j, g_j | a_i, g_i, i \to j) && \\\nonumber
&= d_i \sum_{a_j,g_j} \prob( j \in \mathcal{G}_k | a_j, g_j) \rho_{(a_i,g_i)(a_j,g_j)} && \\\nonumber
&= d_i \sum_{a_j,g_j} \rho_{(a_i,g_i)(a_j,g_j)} \prob( j \in \mathcal{G}_k | a_j, g_j)  && \\\nonumber
&= d_i \sum_{a_j,g_j} \rho_{(a_i,g_i)(a_j,g_j)} \biggl( \frac{N_{k,a_j,g_j}}{N_{a_j,g_j}} \biggr)
\end{align}

\noindent Here $\rho_{(a_i,g_i)(a_j,g_j)}$ is then a latent variable that can be inferred as the mixing rate between egos of age category $a_i$, gender $g_i$ and alters of age category $a_j$, gender $g_j$.

\subsection{Issues}
In our experiments, the non-random age-gender mixing model suffered from bias/variance and identifiability issues because the mixing rate parameters lacked constraints (other than summing to 1). In an effort to resolve this issue, we now propose a model with more structure and fewer parameters.

% MODEL %
\section{Kernel Model}
In this new approach, we first assume that age is continuous ($a_i \in (-\infty,\infty)$) rather than just binning age into categories. This allows us to model the mixing rate of an ego with age $a_i$ with an alter of age $a_j$ as a Gaussian kernel defined smoothly over all possible $a_j$.\\

\noindent Additionally, we also model the alter degree $d_j$ for the first time. Interestingly, this modeling yields different results depending on whether we set up the model from the perspective of the alter or from the perspective of the ego (as we've done in all previous models above). 

%% EGO PERSPECTIVE %%
\pagebreak
\subsection{Ego Perpsective}

\noindent Modifying the derivation from 1.3, the mean can be derived as follows:
\begin{align}
\mu_{ik} 
&= d_i \prob( j \in \mathcal{G}_k | a_i, g_i, i \to j) && \\\nonumber
&= d_i \sum_{g_j} \int_{a_j} \int_{d_j} \prob( j \in \mathcal{G}_k, a_j, g_j, d_j | a_i, g_i, i \to j) dd_j da_j && \\\nonumber
&= d_i \sum_{g_j} \int_{a_j} \int_{d_j} \prob( j \in \mathcal{G}_k | a_j, g_j, d_j, a_i, g_i, i \to j) p(a_j, g_j, d_j | a_i, g_i, i \to j) dd_j da_j && \\\nonumber
&= d_i \sum_{g_j} \int_{a_j} \int_{d_j} \prob( j \in \mathcal{G}_k | a_j, g_j) p(d_j | a_j, g_j, a_i, g_i, i \to j) 
p(a_j | g_j, a_i, g_i, i \to j) p(g_j | a_i, g_i, i \to j) dd_j da_j && \\\nonumber
&= d_i \sum_{g_j} \int_{a_j}  \prob( j \in \mathcal{G}_k | a_j, g_j) \biggl( \int_{d_j}  p(d_j | a_j, g_j, a_i, g_i, i \to j) dd_j \biggr)
p(a_j | g_j, a_i, g_i, i \to j) p(g_j | g_i, i \to j) da_j && \\\nonumber
&= d_i \sum_{g_j} \int_{a_j}  \prob( j \in \mathcal{G}_k | a_j, g_j) 
p(a_j | g_j, a_i, g_i, i \to j) p(g_j | g_i, i \to j) da_j && \\\nonumber
&= d_i \sum_{g_j} \int_{a_j} 
\biggl( \frac{ \int_{a} \prob( j \in \mathcal{G}_k | a, g_j) da }{ \int_{a} \prob( j \in \mathcal{G}_k | a, g_j) da } \biggr) 
\prob( j \in \mathcal{G}_k | a_j, g_j) p(a_j | a_i, g_i, g_j, i \to j) p(g_j | g_i, i \to j) da_j && \\\nonumber
&= d_i \sum_{g_j} p(g_j | g_i, i \to j) 
\biggl( \int_{a} \prob( j \in \mathcal{G}_k | a, g_j) da \biggr) 
\int_{a_j} \biggl( \frac{ \prob( j \in \mathcal{G}_k | a_j, g_j) }{ \int_{a} \prob( j \in \mathcal{G}_k | a, g_j) da } \biggr)
p(a_j | a_i, g_i, g_j, i \to j) da_j && \\\nonumber
&\approx d_i \sum_{g_j} \rho_{g_ig_j} 
\biggl( \sum_{a} \prob( j \in \mathcal{G}_k | a, g_j) \biggr) 
\int_{a_j} \normal(a_j | \mu_{g_j,k}, \sigma_{g_j,k}^2)
\normal(a_j | a_i, \lambda_{g_ig_j}) da_j && \\\nonumber
&= d_i \sum_{g_j} \rho_{g_ig_j} 
\biggl( \sum_{a_j} \frac{ N_{k, a_j, g_j} }{ N_{a_j, g_j} } \biggr) 
\frac{ e^{ -\frac{ (a_i - \mu_{g_j,k})^2 }{ 2(\lambda_{g_ig_j} + \sigma_{g_j,k}^2) } } }{ \sqrt{ 2\pi(\lambda_{g_ig_j} + \sigma_{g_j,k}^2) } }
\end{align}

\noindent Here $\rho_{g_ig_j}$ is the same latent variable as in 1.2 that can be inferred as the mixing rate between egos of gender $g_i$ and alters of gender $g_j$. Additionally, $\lambda_{g_ig_j}$ is a latent variable that can be inferred as the kernel bandwidth of the age mixing kernel. Essentially, small values of $\lambda_{g_ig_j}$ indicate that egos of gender $g_i$ only know alters of gender $g_j$ that are close to the ego in age (whereas larger values indicate the egos know alters of a wide range of ages, not necessarily just those close in age).\\

\noindent Additionally, $\mu_{g_j,k}$ and $\sigma_{g_j,k}^2$ are just estimated from the population data about group $G_k$. These values are analogous (though not exactly equal) to the mean and standard deviation of the ages of alters in group $G_k$ with gender $g_j$.


%SIMULATION%
\pagebreak
\section{Simulation}
\subsection{Data}
We simulate responses to questions about 12 names using estimated age means/variances (for each name), degree regression coefficients, simulated respondent degrees, name overdispersions, kernel lengthscales, and gender mixing rates in an attempt to determine whether our model can recover the parameters. 

$$
\beta = [6.256, -0.005, -3] 
\hspace{20 pt}
\eta = 0.5 
\hspace{20 pt}
d_j \sim \lognormal(\beta_1 + \beta_2 g_j - e^{\beta_3} (a_j - \bar{a})^2, \eta^2)
$$

\noindent \begin{tabular}{c | cccccccccccc} 
Var & Linda & Jen. & Karen & Kim. & Emily & Steph. & Mark & Jacob & Kevin & Kyle & Adam & Bruce \\
\hline
$\mu_k$ & 63.3 & 37.4 & 56.1 & 39.8 & 28.0 & 35.6 & 49.2 & 22.5 & 38.8  & 25.6 & 31.0 & 62.4 \\
$\sigma_k$ & 10.4 & 10.6 & 13.9 & 13.0 & 22.9 & 14.3 & 14.9 & 18.0 & 16.2 & 10.8 & 16.2 & 16.7  \\
$\omega_k$ & 4.23 & 8.42 & 7.65 & 3.83 & 9.79 & 10.91 & 5.01 & 2.80 & 2.22 & 2.60 & 12.69 & 4.95  \\
\end{tabular}\\

$$ \lambda
= \left( \begin{array}{cc} \lambda_{FF} & \lambda_{FM} \\
\lambda_{MF} & \lambda_{MM} \end{array} \right) 
= \left( \begin{array}{cc}
225 & 144 \\
100 & 256 \end{array} \right)
\hspace{20 pt}
\rho
= \left( \begin{array}{cc} \rho_{FF} & \rho_{FM} \\
\rho_{MF} & \rho_{MM} \end{array} \right) 
= \left( \begin{array}{cc}
0.6 & 0.4 \\
0.45 & 0.55 \end{array} \right) $$ \vspace{7 pt}

%PRIORS%
\subsection{Priors}
\noindent The following priors were used:

$$
\beta \sim \normal(0, 2I)
\hspace{15 pt}
\eta \sim \lognormal(-0.7, 0.1)
\hspace{15 pt}
d_j \sim \lognormal(\beta_1 + \beta_2 g_j - e^{\beta_3} (a_j - \bar{a})^2, \eta^2)
$$
$$
\rho_{g_i\cdot} \sim Alpha(5,5)
\hspace{15 pt}
\lambda_{g_ig_j} \sim \lognormal(\log 100, 0.5)
\hspace{15 pt}
\frac{1}{ \frac{1}{\omega_k}+1} \sim Beta(4.5,0.5)
$$

%RESULTS%
% DEGREE
\pagebreak
\subsection{Results}
\subsubsection*{Respondent Degrees ($d_i$)}
\noindent The degrees are recovered well.

\begin{center}
\includegraphics[scale = 0.34]{fig/fit_deg_reg.png}
\end{center}

% LAMBDA
\subsubsection*{Kernel Lengthscale ($\lambda_{g_ig_j}$)}
The continuous model does a decent job of recovering the kernel lengthscales.

\begin{center}
\includegraphics[scale = 0.34]{fig/fit_lambda_reg.png}
\end{center}

%MIXING%
\pagebreak
\subsubsection*{Gender Mixing Rates ($\rho_{g_ig_k}$)}
\noindent the gender mixing rates are recovered decently well.

\begin{center}
\includegraphics[scale = 0.34]{fig/fit_mix_reg.png}
\end{center}

%BETA%
\subsubsection*{Degree Regression Parameters ($\beta_j$)}
\noindent Lastly, the beta regression parameters are recovered very well.

\begin{center}
\includegraphics[scale = 0.34]{fig/fit_beta_reg.png}
\end{center}

% OMNI DATA %
\pagebreak
\section{Omni Data Results}
\subsubsection*{Respondent Degrees}
The continuous model estimates imply decreasing and then increasing network size by age for men, but monotonically decreasing network size by age for women.

\begin{center}
\includegraphics[scale = 0.36]{fig/deg_name_sexage.png}
\end{center}

\subsubsection*{Gender Mixing Rates}
\noindent The gender mixing rates imply male-dominated networks. In general, about 56\% of a female's network is female, while 59\% of a male's network is male. This also implies that females mix more with males than the other way around.

$$ \rho_{BAYES}
= \left( \begin{array}{cc} \rho_{FF} & \rho_{FM} \\
\rho_{MF} & \rho_{MM} \end{array} \right) 
= \left( \begin{array}{cc}
0.44 & 0.56 \\
0.41 & 0.59 \end{array} \right) $$

% KERNEL LENTGHSCALES
\pagebreak
\subsubsection*{Kernel Lengthscales}
\noindent The length scales estimated from the actual data are large, implying very flat kernels. The important distinction here, then, is perhaps the relative size of the lengthscales.

$$ \lambda_{BAYES}
= \left( \begin{array}{cc} \lambda_{FF} & \lambda_{FM} \\
\lambda_{MF} & \lambda_{MM} \end{array} \right) 
= \left( \begin{array}{cc}
969 & 1253 \\
1104 & 1153 \end{array} \right) $$ \vspace{7 pt}

\noindent Indeed, it seems that the female to female kernel is much tighter than the male to male kernel, implying that women tend to know a narrow age range of other women but men tend to know a wide age range of other men. Alternatively, the female to male kernel is much wider than the male to female kernel, implying that women know a wider age range of men while men known a relatively narrow age range of women.

\begin{center}
\includegraphics[scale = 0.36]{fig/kern_name_sex.png}
\end{center}

\end{document}